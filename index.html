<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Occ3D: A Large-Scale 3D Occupancy Prediction Benchmark for Autonomous Driving.">
  <meta name="keywords" content="Occupancy, Autonomous Driving">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Occ3D: A Large-Scale 3D Occupancy Prediction Benchmark for Autonomous Driving</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/MARSLAB.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Occ3D: A Large-Scale 3D Occupancy Prediction Benchmark for Autonomous Driving</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Xiaoyu Tian</a><sup>1,†</sup>,</span>
            <span class="author-block">
              Tao Jiang</a><sup>1,†</sup>,</span>
            <span class="author-block">
              Longfei Yun</a><sup>1</sup>,</span>
            <span class="author-block">
              Yucheng Mao</a><sup>1</sup>,</span>
            <span class="author-block">
              Huitong Yang</a><sup>1</sup>
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=v-AEFIEAAAAJ&hl=en">Yue Wang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com.hk/citations?user=nUyTDosAAAAJ&hl=en">Yilun Wang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=DmahiOYAAAAJ&hl=en">Hang Zhao</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>IIIS, Tsinghua University</span>
            <span class="author-block"><sup>2</sup>NVIDIA</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2304.14365"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Tsinghua-MARS-Lab/Occ3D.git"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/drive/folders/1wZ-8OI1IJkrXo6BudFSGmaKXBUYQ3ts_?usp=share_link"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Occ3D-nuScenes</span>
                  </a>
                  <span class="link-block">
                    <a href="https://drive.google.com/drive/folders/13WxRl9Zb_AshEwvD96Uwz8cHjRNrtfQk?usp=share_link"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <i class="far fa-images"></i>
                      </span>
                      <span>Occ3D-Waymo</span>
                      </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item seq 010">
          <video poster="" id="000" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/521.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item seq 020">
          <video poster="" id="000" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/711.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item seq 150">
          <video poster="" id="000" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/721.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item seq 150">
          <video poster="" id="000" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/521.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!--  Introduction. -->
    <div class="columns is-centered">
      <div class="column is-full-width">

        <br/>
        <!--/ Interpolating. -->

        <h2 class="title is-3"> Introduction</h2>
        <div class="content has-text-justified">
          <p>
            One of the most popular visual perception tasks is 3D object detection, their expressiveness is still limited as shown in Figure1:
            (1) 3D bounding box representation erases the geometric details of objects, eg a bendy bus or a construction vehicle;
            (2) rarely seen objects, like trash or tree branch on the streets, are often ignored and not label in existing datasets.
          </p>

          <div class="columns is-vcentered interpolation-panel">
            <div class="column is-30 has-text-centered">
              <img src="./static/images/Occ3D_teaser.png"
                   class="interpolation-image"
                   alt="Interpolate start reference image."/>
              <p>Figure 1:The Occ3D dataset demonstrates rich semantic and geometric expressiveness. 
              </p>
            </div>
          </div>
          <!--/ Re-rendering. -->
          <p>
            We formalize the 3D occupancy prediction task as follows: a model needs to jointly estimate the occupancy state and semantic label of every voxel in the scene from images. 
            The occupancy state of each voxel can be categorized as free, occupied, or unobserved. 
            For occupied voxels, semantic labels are assigned. For objects that are not in the predefined categories, they are labeled as General Objects (GOs).
          </p>

        </div>


        <h3 class="title is-3"></h3>
        

        <h2 class="title is-3"> CVPR2023 Occupancy Prediction Challenge</h2>
        <div class="content has-text-justified">
          <p>
            To promote the 3D occupancy prediction task, we co-host a 3D Occupancy Prediction Challenge in CVPR2023.
            For more information about the challenge, please refer to <a href="https://github.com/CVPR2023-3D-Occupancy-Prediction/CVPR2023-3D-Occupancy-Prediction">the Challenge Page</a>.
          </p>
        </div>

        
        <h3 class="title is-3">The Occ3D Dataset</h3>
        
        <!-- Interpolating. -->
        <h3 class="title is-4">Dataset Downloads</h3>
        <div class="content has-text-justified">
          <p>
            We generate two 3D occupancy prediction datasets, <a href="https://drive.google.com/drive/folders/13WxRl9Zb_AshEwvD96Uwz8cHjRNrtfQk">Occ3D-Waymo</a>
            and <a href="https://drive.google.com/drive/folders/13WxRl9Zb_AshEwvD96Uwz8cHjRNrtfQk?usp=share_link">Occ3D-nuScenes</a>.
            Checkout our <a href="https://github.com/Tsinghua-MARS-Lab/Occ3D.git">code and toolkit</a> to get started using our datasets. 
            Before using the dataset, you should agree to the terms of use of the
             <a href="https://www.nuscenes.org/">nuScenes</a> and 
             <a href="https://waymo.com/open/">Waymo</a>.
             The code used to generate the data and the generated data are both subject to the <a href="https://en.wikipedia.org/wiki/MIT_License">MIT License</a>.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-30 has-text-centered">
            <img src="./static/images/Dataset_new_statics.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Table 1: Comparing Occ3D Datasets with other datasets for 3D perception.</p>
          </div>
        </div>
        <br/>
        <!--/ Interpolating. -->
        
        
        <!-- Re-rendering. -->
        <h3 class="title is-4">Dataset Construction Pipeline</h3>
        <div class="content has-text-justified">
          <p>
            Acquiring dense voxel-level annotations for a 3D scene
            can be challenging and impractical. To address this, we
            propose a semi-automatic label generation pipeline that utilizes existing labeled 3D perception datasets. 
          </p>

          <div class="columns is-vcentered interpolation-panel">
            <div class="column is-30 has-text-centered">
              <img src="./static/images/pipeline_v3.pdf"
                   class="interpolation-image"
                   alt="Interpolate start reference image."/>
              <p>Figure 2: Our label generation pipeline.</p>
            </div>
          </div>
          <!--/ Re-rendering. -->
          <p>
            Our proposed label generation pipeline addresses the above challenges, an overview is shown in Figure 2. Initially, in voxel densification, we increase the density of the point clouds by performing multi-frame aggregation for both static and dynamic objects separately. Then we employ a K-nearest neighbor algorithm to assign labels to unlabeled points and utilize mesh reconstruction to perform hole-filling. 
            Subsequently, we carry out occlusion reasoning from both LiDAR and camera perspectives, utilizing a ray-casting operation to label the occupancy state of each voxel. Finally, misaligned voxels are eliminated through an image-guided voxel refinement process.
          </p>
        </div>
      

        <h3 class="title is-4">Quality Check</h3>
        <div class="content has-text-justified">
          <p>
            Acquiring an occupancy representation that adheres to the complete shape of all objects is challenging.
            Therefore, evaluating the quality of the dataset and ensuring the effectiveness of each step in our pipeline is critical. 
          </p>

          <div class="columns is-vcentered interpolation-panel">
            <div class="column is-30 has-text-centered">
              <img src="./static/images/quality_check.png"
                    class="interpolation-image"
                    alt="Interpolate start reference image."/>
              <p>Figure 4: 3D-2D consistency.</p>
            </div>
          </div>
          
          <p>
            Our proposed label generation pipeline addresses the above challenges, an overview is shown in Figure 2. Initially, in voxel densification, we increase the density of the point clouds by performing multi-frame aggregation for both static and dynamic objects separately. Then we employ a K-nearest neighbor algorithm to assign labels to unlabeled points and utilize mesh reconstruction to perform hole-filling. 
            Subsequently, we carry out occlusion reasoning from both LiDAR and camera perspectives, utilizing a ray-casting operation to label the occupancy state of each voxel. Finally, misaligned voxels are eliminated through an image-guided voxel refinement process.
          </p>

      </div>

        <!-- Re-rendering. -->
        <h3 class="title is-3">Methods</h3>
        <div class="content has-text-justified">
          <p>
            To deal with the challenging 3D occupancy prediction problem, we present a new transformer-based model named Coarse-to-Fine Occupancy (CTF-Occ).
            First, 2D image features are extracted from multi-view images with an image backbone. Then, 3D voxel
            queries aggregate 2D image features into 3D space via a
            cross-attention operation. Our approach involves using a
            pyramid voxel encoder that progressively improves voxel
            feature representations through incremental token selection
            and spatial cross-attention in a coarse-to-fine fashion. This
            method enhances the spatial resolution and fine-tunes the
            detailed geometry of objects, ultimately leading to more accurate 3D occupancy predictions.
          </p>

          <div class="columns is-vcentered interpolation-panel">
            <div class="column is-30 has-text-centered">
              <img src="./static/images/occ_overall.png"
                   class="interpolation-image"
                   alt="Interpolate start reference image."/>
              <p>Figure 3: The model architecture of CTF-Occ.</p>
            </div>
          </div>
          <!--/ Re-rendering. -->
          <p>
            An overview of CTF-Occ network is shown in Figure 3. First, 2D image features are extracted from multi-view images with an image backbone.
            Then, 3D voxel queries aggregate 2D image features into 3D space via a cross-attention operation.
          </p>

      </div>

        <h3 class="title is-3">Experiments</h3>
        <div class="content has-text-justified">
          <p>
            Our method outperforms previous methods on Occ3D-Nuscenes and Occ3D-Waymo dataset.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-30 has-text-centered">
            <img src="./static/images/Occ3D-nuscenes_new.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <!-- <p>3D occupancy prediction performance on the Occ3D-nuScenes dataset. C, L denote camera and LiDAR.</p>  -->
            <img src="./static/images/Occ3D-Waymo_new.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
                 <p>Table 2: 3D occupancy prediction performance on the Occ3D-nuScenes and Occ3D-Waymo dataset. Cons. Veh represents
                  construction vehicle and Dri. Sur is for driveable surface.</p>
          </div>
        </div>
        <br/>
      </div>
    </div>
    <!--/ Animation. -->


    <!-- Concurrent Work. -->
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

      </div>
    </div> -->
    <!--/ Concurrent Work. -->
    

  <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
<pre><code>@article{tian2023occ3d,
  title={Occ3D: A Large-Scale 3D Occupancy Prediction Benchmark for Autonomous Driving},
  author={Tian, Xiaoyu and Jiang, Tao and Yun, Longfei and Wang, Yue and Wang, Yilun and Zhao, Hang},
  journal={arXiv preprint arXiv:2304.14365},
  year={2023}
}</code></pre>
  </div>
  </section>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>. <a href="https://github.com/nerfies/nerfies.github.io"> Page Template </a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


</body>
</html>
