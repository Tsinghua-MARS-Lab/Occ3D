<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Occ3D: A Large-Scale 3D Occupancy Prediction Benchmark for Autonomous Driving.">
  <meta name="keywords" content="Occupancy, Autonomous Driving">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Occ3D: A Large-Scale 3D Occupancy Prediction Benchmark for Autonomous Driving</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Occ3D: A Large-Scale 3D Occupancy Prediction Benchmark for Autonomous Driving</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Xiaoyu Tian</a><sup>1,†</sup>,</span>
            <span class="author-block">
              Tao Jiang</a><sup>1,†</sup>,</span>
            <span class="author-block">
              Longfei Yun</a><sup>1,†</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=v-AEFIEAAAAJ&hl=en">Yue Wang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com.hk/citations?user=nUyTDosAAAAJ&hl=en">Yilun Wang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=DmahiOYAAAAJ&hl=en">Hang Zhao</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>IIIS, Tsinghua University</span>
            <span class="author-block"><sup>2</sup>NVIDIA</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/XX"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/XX"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Tsinghua-MARS-Lab/Occ3D.git"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/drive/folders/1wZ-8OI1IJkrXo6BudFSGmaKXBUYQ3ts_?usp=share_link"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Occ3D-nuScenes</span>
                  </a>
                  <span class="link-block">
                    <a href="https://drive.google.com/drive/folders/13WxRl9Zb_AshEwvD96Uwz8cHjRNrtfQk?usp=share_link"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <i class="far fa-images"></i>
                      </span>
                      <span>Occ3D-Waymo</span>
                      </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item seq 000">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/seq000.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item seq 010">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/seq000.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item seq 020">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/seq000.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item seq 050">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/seq000.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item seq 150">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/seq000.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!--  Introduction. -->
    <div class="columns is-centered">
      <div class="column is-full-width">

        <br/>
        <!--/ Interpolating. -->

        <h2 class="title is-3"> Introduction</h2>
        <div class="content has-text-justified">
          <p>
            3D perception is a crucial component in vision-based robotic systems like autonomous driving. One of the most popular visual perception tasks is 3D object detection, which estimates the locations and dimensions of objects defined in a fixed ontology tree.
            While the outputs are concise 3D bounding boxes that can be used by downstream tasks, their expressiveness is still limited as shown in Figure1:
            (1) 3D bounding box representation erases the geometric details of objects, eg a bendy bus has two or more sections connected by rotating joints, a construction vehicle has a mechanical arm that protrudes from the main body;
            (2) rarely seen objects, like trash or tree branch on the streets, are often ignored and not label in existing datasets since object categories cannot be extensively enumerated in the ontology tree.
          </p>

          <div class="columns is-vcentered interpolation-panel">
            <div class="column is-30 has-text-centered">
              <img src="./static/images/Occ3D_teaser.png"
                   class="interpolation-image"
                   alt="Interpolate start reference image."/>
              <p>Figure 1:The Occ3D dataset demonstrates rich semantic and geometric expressiveness. 
              </p>
            </div>
          </div>
          <!--/ Re-rendering. -->
          <p>
            In this work, we define a comprehensive 3D scene understanding task for vision-based autonomous driving, dubbed 3D Occupancy Prediction.
            3D occupancy prediction jointly estimates the occupancy state and semantic label of every voxel in the scene from multi-view images. 
            The occupancy state of each voxel can be "free", "occupied", or "unobserved".
            Having an unobserved label for voxels is crucial in 3D occupancy prediction to account for visibility and exclude unobserved voxels.
            Semantic labels are estimated for occupied voxels based on their occupancy state.
            For objects with predefined categories in the dataset, their semantic labels correspond to their respective categories. 
            Conversely, objects that are not categorized are labeled as General Objects (GO). 
            Although GOs are seldom encountered, they are essential for perception tasks that require safety considerations since they are typically undetected by 3D object detection with predefined categories.
          </p>

        </div>


        <h3 class="title is-3">CVPR2023 Occupancy Prediction Challenge</h3>
        We co-host an Occupancy Prediction Challenge in CVPR2023.
        For more information about the challenge, please refer <a href="https://github.com/CVPR2023-3D-Occupancy-Prediction/CVPR2023-3D-Occupancy-Prediction">HERE</a>.
        
        <h3 class="title is-3">The Occ3D Dataset</h3>
        
        <!-- Interpolating. -->
        <h3 class="title is-4">Dataset Downloads</h3>
        <div class="content has-text-justified">
          <p>
            We generate two 3D occupancy prediction datasets, <a href=>Occ3D-Waymo</a> and <a href="https://drive.google.com/drive/folders/13WxRl9Zb_AshEwvD96Uwz8cHjRNrtfQk?usp=share_link">Occ3D-nuScenes</a>, statistics are shown below.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-30 has-text-centered">
            <img src="./static/images/Dataset_statistics.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Table 1: Comparing Occ3D Datasets with other datasets for 3D perception.</p>
          </div>
        </div>
        <br/>
        <!--/ Interpolating. -->
        
        
        <!-- Re-rendering. -->
        <h3 class="title is-4">Dataset Construction Pipeline</h3>
        <div class="content has-text-justified">
          <p>
            Acquiring dense voxel-level annotations for a 3D scene
            can be challenging and impractical. To address this, we
            propose a semi-automatic label generation pipeline that utilizes existing labeled 3D perception datasets. 
          </p>

          <div class="columns is-vcentered interpolation-panel">
            <div class="column is-30 has-text-centered">
              <img src="./static/images/dataset_pipeline_v2_.png"
                   class="interpolation-image"
                   alt="Interpolate start reference image."/>
              <p>Figure 2: Our label generation pipeline.</p>
            </div>
          </div>
          <!--/ Re-rendering. -->
          <p>
            <b>Data preparation. </b> Our label generation pipeline requires a 3D dataset where each scene contains the following sensor data: 
            i) a (multi-view) camera image sequence; ii) a 3D LiDAR point cloud sequence; iii) a 3D pose sequence from IMU. 
            The intrinsic and extrinsic parameters of all the cameras and LiDARs are also required for coordinate transformation and projection. 
            Additionally, we require human-annotated box-level semantic labels of common objects, and optionally point-level semantic labels. 
          </p>
  
          <p>
            <b>Point cloud aggregation. </b> 
            We process dynamic objects and static objects separately. 
            Points of dynamic objects are transformed and aggregated according to bounding box annotations at each frame and ego poses between different frames.
            For points of static objects, we simply aggregate them based on ego poses. 
            Regarding the points in the unannotated frames that are not enclosed by bounding boxes, we use K-Nearest Neighbor to perform major voting to determine their semantic labels. 
          </p>
          
          <p>
            <b>LiDAR visibility. </b> 
            Since LiDAR points are sparse, some occupied voxels are not scanned by LiDAR beams, and can be mislabeled as "free". 
            To avoid this issue, we perform a ray casting operation to determine the visibility of each voxel. 
            Concretely, we connect each LiDAR point with the sensor origin to form a ray, a voxel is visible if it reflects LiDAR points ("occupied") or is traversed through by a ray ("free"); 
            Otherwise, it is tagged as "unobserved". 
          </p>
  
          <p>
            <b>Camera visibility. </b> 
            Since we focus on a vision-centric task, we further generate a camera visibility mask, indicating whether each voxel is observed in the present multi-camera views.
            Concretely, for each camera view, we connect each occupied voxel center with the camera center and form a ray. 
            Along each ray, we set the voxels before and including the first occupied voxel as "observed", and the rest "unobserved".
            Voxels not scanned by any camera rays are marked as "unobserved" as well.
          </p>

      </div>

        <!-- Re-rendering. -->
        <h3 class="title is-3">Methods</h3>
        <div class="content has-text-justified">
          <p>
            To deal with the challenging 3D occupancy prediction problem, we present a new transformer-based model named Coarse-to-Fine Occupancy (CTF-Occ).
            First, 2D image features are extracted from multi-view images with an image backbone. Then, 3D voxel
            queries aggregate 2D image features into 3D space via a
            cross-attention operation. Our approach involves using a
            pyramid voxel encoder that progressively improves voxel
            feature representations through incremental token selection
            and spatial cross-attention in a coarse-to-fine fashion. This
            method enhances the spatial resolution and fine-tunes the
            detailed geometry of objects, ultimately leading to more accurate 3D occupancy predictions.
          </p>

          <div class="columns is-vcentered interpolation-panel">
            <div class="column is-30 has-text-centered">
              <img src="./static/images/occ_overall.png"
                   class="interpolation-image"
                   alt="Interpolate start reference image."/>
              <p>Figure 3: The model architecture of CTF-Occ.</p>
            </div>
          </div>
          <!--/ Re-rendering. -->
          <p>
            <b>Incremental Token Selection. </b> 
            Given that most 3D voxel grids in a scene are empty, we propose an incremental token selection strategy that selectively chooses foreground and uncertain voxel tokens in cross-attention computation. 
            This strategy enables adaptive and efficient computation without sacrificing accuracy.
            Specifically, at the beginning of each pyramid level, each voxel token is fed into a binary classifier to predict whether this voxel is empty or not. 
            We use the binary ground-truth occupancy map as supervision to train the classifier.
            In our approach, we select either the K-most uncertain voxels, which are those with probabilities near 0.5, 
            or K non-empty voxels with the highest scores, or we combine both types of voxels with a specific percentage.
          </p>
  
          <p>
            <b>Spatial Cross Attention. </b> 
            At every level of the pyramid, we first select the top-K voxel tokens and then aggregate the corresponding image features. 
            Subsequently, we apply spatial cross-attention to further refine the voxel features.
          </p>

          <p>
            <b>Convolutional Feature Extractor. </b> 
            Once we apply deformable cross-attention to the relevant image features, 
            we proceed to update the features of the foreground voxel tokens. 
            Then, we use a series of stacked convolutions to enhance feature interaction throughout the entire 3D voxel feature maps. 
            At the end of the current level, we upsample the 3D voxel features using trilinear interpolation.
          </p>

      </div>

        <!-- Interpolating. -->
        <h3 class="title is-3">Experiments</h3>
        <div class="content has-text-justified">
          <p>
            Our method outperforms previous methods on Occ3D-Waymo dataset, i.e. increasing the mIoU by 3.11. Especially for some small objects such as pedestrians and bicycles, our method surpasses the baseline method by 4.11 and 13.0 IoU respectively.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-30 has-text-centered">
            <img src="./static/images/exp.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <!-- <p>3D occupancy prediction performance on the Occ3D-nuScenes dataset. C, L denote camera and LiDAR.</p>  -->
          </div>
        </div>
        <br/>
        <!--/ Interpolating. -->


      </div>
    </div>
    <!--/ Animation. -->


    <!-- Concurrent Work. -->
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

      </div>
    </div> -->
    <!--/ Concurrent Work. -->
    

  <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{occ3d,
      author    = {Xiaoyu Tian, Tao Jiang, Longfei Yun, Yue Wang, Yilun Wang, Hang Zhao},
      title     = {Occ3D: A Large-Scale 3D Occupancy Prediction Benchmark for Autonomous Driving},
      year      = {2023},}
    </code></pre>

  <section class="section" id="Citation">

  <div class="container is-max-desktop content">
    <h2 class="title is-3">Citation</h2>
    <p>[1] Iro Armeni, Sasha Sax, Amir R Zamir, and Silvio Savarese.
Joint 2d-3d-semantic data for indoor scene understanding.
arXiv preprint arXiv:1702.01105, 2017. 2</p>
    <p>[2] Jens Behley, Martin Garbade, Andres Milioto, Jan Quenzel,
Sven Behnke, Cyrill Stachniss, and Juergen Gall. A dataset
for semantic segmentation of point cloud sequences. arXiv
preprint arXiv:1904.01416, 2(3), 2019. 2 </p>

<p>[3] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke,
C. Stachniss, and J. Gall. SemanticKITTI: A Dataset for
Semantic Scene Understanding of LiDAR Sequences. In
Proc. of the IEEE/CVF International Conf. on Computer
Vision (ICCV), 2019. 3</p>
<p>[4] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora,
Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-
ancarlo Baldan, and Oscar Beijbom. nuscenes: A multi-
modal dataset for autonomous driving. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, pages 11621–11631, 2020. 1, 2, 3</p>
<p>[5] Anh-Quan Cao and Raoul de Charette. Monoscene: Monocu-
lar 3d semantic scene completion. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 3991–4001, 2022. 7</p>
<p>[6] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej
Halber, Matthias Niessner, Manolis Savva, Shuran Song,
Andy Zeng, and Yinda Zhang. Matterport3d: Learning
from rgb-d data in indoor environments. arXiv preprint
arXiv:1709.06158, 2017. 2</p>
<p>[7] Xuanyao Chen, Tianyuan Zhang, Yue Wang, Yilun Wang, and
Hang Zhao. Futr3d: A unified sensor fusion framework for
3d detection. arXiv preprint arXiv:2203.10642, 2022. 2 </p>
<p>[8] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-
ber, Thomas Funkhouser, and Matthias Nießner. Scannet:
Richly-annotated 3d reconstructions of indoor scenes. In
Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 5828–5839, 2017. 2</p>
<p>[9] Zhipeng Ding, Xu Han, and Marc Niethammer. Votenet: A
deep learning label fusion method for multi-atlas segmenta-
tion. In International Conference on Medical Image Com-
puting and Computer-Assisted Intervention, pages 202–210.
Springer, 2019. 2</p>
<p>[10] Hugh Durrant-Whyte and Tim Bailey. Simultaneous local-
ization and mapping: part i. IEEE robotics & automation
magazine, 13(2):99–110, 2006. 4</p>
<p>[11] Michael Firman, Oisin Mac Aodha, Simon Julier, and
Gabriel J Brostow. Structured prediction of unobserved voxels
from a single depth image. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
5431–5440, 2016. 3</p>
<p>[12] Saurabh Gupta, Ross Girshick, Pablo Arbel  ́aez, and Jitendra
Malik. Learning rich features from rgb-d images for object</p>
detection and segmentation. In Computer Vision–ECCV 2014:
13th European Conference, Zurich, Switzerland, September 6-
12, 2014, Proceedings, Part VII 13, pages 345–360. Springer,
2014. 3</p>
<p>[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016. 7</p>
<p>[14] Junjie Huang, Guan Huang, Zheng Zhu, and Dalong Du.
Bevdet: High-performance multi-camera 3d object detection
in bird-eye-view. arXiv preprint arXiv:2112.11790, 2021. 7,
8</p>
<p>[15] Yuanhui Huang, Wenzhao Zheng, Yunpeng Zhang, Jie
Zhou, and Jiwen Lu. Tri-perspective view for vision-
based 3d semantic occupancy prediction. arXiv preprint
arXiv:2302.07817, 2023. 2</p>
<p>[16] Hyeong-Seok Jeon, Dong-Suk Kum, and Woo-Yeol Jeong.
Traffic scene prediction via deep learning: Introduction of
multi-channel occupancy grid map as a scene representation.
In 2018 IEEE Intelligent Vehicles Symposium (IV), pages
1496–1501. IEEE, 2018. 2</p>
<p>[17] Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou,
Jiong Yang, and Oscar Beijbom. PointPillars: Fast Encoders
for Object Detection from Point Clouds. In CVPR, pages
12697–12705, 2019. 2</p>
<p>[18] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chong-
hao Sima, Tong Lu, Qiao Yu, and Jifeng Dai. Bevformer:
Learning bird’s-eye-view representation from multi-camera
images via spatiotemporal transformers. arXiv preprint
arXiv:2203.17270, 2022. 1, 2, 7, 8</p>
<p>[19] Yiyi Liao, Jun Xie, and Andreas Geiger. KITTI-360: A novel
dataset and benchmarks for urban scene understanding in 2d
and 3d. arXiv preprint arXiv:2109.13410, 2021. 3</p>
<p>[20] Yiyi Liao, Jun Xie, and Andreas Geiger. Kitti-360: A novel
dataset and benchmarks for urban scene understanding in 2d
and 3d. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 2022. 2</p>
<p>[21] Yingfei Liu, Tiancai Wang, Xiangyu Zhang, and Jian Sun.
Petr: Position embedding transformation for multi-view 3d
object detection. arXiv preprint arXiv:2203.05625, 2022. 2</p>
<p>[22] Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang,
Huizi Mao, Daniela Rus, and Song Han. Bevfusion: Multi-
task multi-sensor fusion with unified bird’s-eye view repre-
sentation. arXiv preprint arXiv:2205.13542, 2022. 2</p>
<p>[23] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. arXiv preprint arXiv:1711.05101, 2017. 7</p>
<p>[24] Hans Moravec and Alberto Elfes. High resolution maps from
wide angle sonar. In Proceedings. 1985 IEEE international
conference on robotics and automation, volume 2, pages 116–
121. IEEE, 1985. 2</p>
<p>[25] Charles R Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas J
Guibas. Frustum PointNets for 3D Object Detection from
RGB-D Data. In CVPR, pages 918–927, 2018. 2</p>
<p>[26] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.
PointNet: Deep Learning on Point Sets for 3D Classification
and Segmentation. In CVPR, pages 652–660, 2017. 2 </p>
<p>[27] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J
Guibas. PointNet++: Deep Hierarchical Feature Learning on
Point Sets in a Metric Space. In NeurIPS, pages 5099–5108,
2017. 2</p>
<p>[28] Thomas Roddick and Roberto Cipolla. Predicting seman-
tic map representations from images using pyramid occu-
pancy networks. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 11138–
11147, 2020. 2</p>
<p>[29] Danila Rukhovich, Anna Vorontsova, and Anton Konushin.
Fcaf3d: fully convolutional anchor-free 3d object detection.
In Computer Vision–ECCV 2022: 17th European Conference,
Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part X,
pages 477–493. Springer, 2022. 2</p>
<p>[30] Abhinav Shrivastava, Abhinav Gupta, and Ross Girshick.
Training region-based object detectors with online hard ex-
ample mining. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 761–769,
2016. 6</p>
<p>[31] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob
Fergus. Indoor segmentation and support inference from rgbd
images. ECCV (5), 7576:746–760, 2012. 3</p>
<p>[32] Andrea Simonelli, Samuel Rota Bulo, Lorenzo Porzi, Manuel
L  ́opez-Antequera, and Peter Kontschieder. Disentangling
monocular 3d object detection. In Proceedings of the
IEEE/CVF International Conference on Computer Vision,
pages 1991–1999, 2019. 2</p>
<p>[33] Liat Sless, Bat El Shlomo, Gilad Cohen, and Shaul Oron.
Road scene understanding by occupancy grid learning from
sparse radar clusters using semantic segmentation. In Proceed-
ings of the IEEE/CVF International Conference on Computer
Vision Workshops, pages 0–0, 2019. 2</p>
<p>[34] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien
Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou,detection and segmentation. In Computer Vision–ECCV 2014:
13th European Conference, Zurich, Switzerland, September 6-
12, 2014, Proceedings, Part VII 13, pages 345–360. Springer,
2014. 3
<p>[35] Sebastian Thrun. Probabilistic robotics. Communications of
the ACM, 45(3):52–57, 2002. 2</p>
<p>[36] Tai Wang, Xinge Zhu, Jiangmiao Pang, and Dahua Lin.
Fcos3d: Fully convolutional one-stage monocular 3d object
detection. arXiv preprint arXiv:2104.10956, 2021. 2, 7
<p>[37] Yue Wang, Vitor Campagnolo Guizilini, Tianyuan Zhang,
Yilun Wang, Hang Zhao, and Justin Solomon. Detr3d: 3d
object detection from multi-view images via 3d-to-2d queries.
In Conference on Robot Learning, pages 180–191. PMLR,
2022. 1, 2</p>
<p>[38] Yue Wang and Justin M. Solomon. Object dgcnn: 3d object
detection using dynamic graphs. In 2021 Conference on
Neural Information Processing Systems (NeurIPS), 2021. 2
[39] Yan Yan, Yuxing Mao, and Bo Li. SECOND: Sparsely Em-
bedded Convolutional Detection. Sensors, 18(10):3337, 2018.
2</p>
<p>[40] Tianwei Yin, Xingyi Zhou, and Philipp Kr  ̈ahenb  ̈uhl. Center-
based 3D Object Detection and Tracking. arXiv preprint
arXiv:2006.11275, 2020. 2</p>
<p>[41] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Bar-
riuso, and Antonio Torralba. Scene parsing through ade20k
dataset. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 633–641, 2017. 4</p>
<p>[42] Yin Zhou and Oncel Tuzel. VoxelNet: End-to-End Learning
for Point Cloud Based 3D Object Detection. In CVPR, pages
4490–4499, 2018. 2</p>

  </div>
  </section>
<!-- </section> -->

<!-- 
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>. Borrow the template from<a href="https://github.com/nerfies/nerfies.github.io"> HERE </a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>
 -->

</body>
</html>
